[TOC]



### JDK基础模块

参考源码分析：[jdk1.8-source](https://github.com/zcswl7961/jdk1.8-source)

- [x] ReentrantLock   [逐行源码分析AbstractQueuedSynchronizer(AQS)中ReentrantLock的源码实现](http://www.zcswl7961.com/index.php/2020/12/03/abstractqueuedsynchronizer/)
- [x] CountDownLatch [逐行源码分析AbstractQueuedSynchronizer(AQS)中CountDownLatch的源码实现](http://www.zcswl7961.com/index.php/2020/12/09/aqs-countdownlatch/)
- [x] Semaphore [逐行源码分析AbstractQueuedSynchronizer(AQS)中Semaphore的源码实现](http://www.zcswl7961.com/index.php/2020/12/09/aqs-semaphore/)
- [x] HashMap [Java包-HashMap](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] TreeMap [Java包-TreeMap](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] LinkedHashMap [Java包-LinkedHashMap](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] ConcurrentHashMap [Java包-ConcurrentHashMap](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] Executor [线程池](https://www.processon.com/view/link/60139f617d9c08426cf87106)
- [x] ThreadPoolExecutor [线程池-ThreadPoolExecutor](https://www.processon.com/view/link/60139f617d9c08426cf87106)
- [x] ScheduledThreadPoolExecutor [线程池-ScheduledThreadPoolExecutor](https://www.processon.com/view/link/60139f617d9c08426cf87106)
- [x] ArrayList [Java包-ArrayList](https://www.processon.com/view/link/60139f617d9c08426cf87106)
- [x] LinkedArrayList [Java包-LinkedList](https://www.processon.com/view/link/60139f617d9c08426cf87106)
- [x] Queue [Java包-Queue和Deque](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] ClassLoader#getResource Class#getResource [Class和ClassLoader关于getResource()，getResourceAsStream()的区别](https://blog.csdn.net/zcswl7961/article/details/103831231)
- [x] Throwable [Java包-Throwable体系](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] ServiceLoader SPI [Java包-ServiceLoader](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] Reference 引用类型 [Java包-Reference](https://www.processon.com/view/link/60139ed4079129652cdf9c93)
- [x] ThreadLocal [ThreadLocal的内存泄漏问题](https://www.zcswl7961.com/index.php/2021/02/03/threadlocal/) [一个面试进行的ThreadLocal源码深入分析](https://blog.csdn.net/zcswl7961/article/details/100769425?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161249075716780261973706%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=161249075716780261973706&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-1-100769425.pc_v1_rank_blog_v1&utm_term=ThreadLocal&spm=1018.2226.3001.4450)
- [x] Thread [线程](https://www.processon.com/view/link/60139f617d9c08426cf87106)



#### LinkedHashMap

​	1，LinkedHashMap是基于HashMap的基础上进行实现

​	2，插入的字段封装成了自己设置的Entry，并且加了一个before 和after，表示当前数据链接的前一个值和后一个值（双端链表）

​	插入的时候都是插入到链表的尾部：

​	**LinkedHashMap构造函数有一个参数：accessOrder：默认的情况下时false：表示LinkedHashMap是按照对应的插入顺序 true：表示访问顺序，如果一个值被访问了之后，会插入到链表的最后面，链表最前面的值表示都是最近未访问的值**

​	使用LinkedHashMap可以作为LRU缓存淘汰算法工具：

```
/**
 * LRU 最近最少使用的操作
 * LinkedHashMap 双端链表的方式
 * @author zhoucg
 * @date 2021-04-07 16:17
 */
public class LRU extends LinkedHashMap<Integer, Integer> {
    private int capacity;

    public LRU(int capacity) {
        super(capacity, 0.75f, true);
    }

    public int get(int key) {
        return super.getOrDefault(key, -1);
    }

    public void put(int key, int value) {
        super.put(key,value);
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<Integer, Integer> eldest) {
        return size() > capacity;
    }
}
```

#### HashMap

​	HashMap的内部的数据结构：数组+链表（链表长度大于8的时候会红黑树）

​	HashMap源码分析是一个很值得分析的点，

##### 1，为什么HashMap的长度是2的n次幂

​	核心得原因就是：**保持hashcode的散列，同时防止hash冲突**  

​	[博客原理](https://www.cnblogs.com/zxporz/p/11204233.html)

这是一个很好得问题，需要去了解出HashMap取位置的算法操作，以及其内部的核心算法：  
**HashMap根据指定的key取hash值得操作：**  

```
return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
```

**HashMap取对应数组槽得算法：**  

```
tab[i = (n - 1) & hash])
```

**HashMap是如何将对应得key存入到指定得数组中（找槽位）**

首先HashMap会根据传入得key值，重新计算出新得hashcode值，计算方法为：  
```
return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
```
即：**如果传入得key为null，返回得hashCode为0**【这个是特殊情况】  
	1，首先会将k得原始得hashcode值无符号得右移16位，即返回的hashcode值（int） 低16位存储的是原hash值得高16位  
	2，然后在和原来的hashcode进行亦或操作。【亦或操作即相同取0， 相反取1】  
	3，**亦或的操作即：保留原来hashcode值得高16位，新得hashcode值得低16位实际上是由原来得高16位与低16位进行亦或得到的，即：可以将高低位二进制特征混合起来（我觉得这个词说的很好）**  
<**为什么要进行亦或操作而不是其他**>
 **异或运算能更好的保留各部分的特征，如果采用&运算计算出来的值会向0靠拢，采用|运算计算出来的值会向1靠拢**

然后，获得的新的hash值会进行：
```
(p = tab[i = (n - 1) & hash])
```
操作获取对应的数组的槽位；

从这个问题也能看出来，**为什么hashMap的数组长度为2的n次幂**

​	**1、为了让哈希后的结果更加均匀**  
​	**2、可以通过位运算e.hash & (newCap - 1)来计算，a % (2^n) 等价于 a & (2^n - 1)  ，位运算的运算效率高于算术运算，原因是算术运算还是会被转化为位运算**


《**这是表面原因，%len和&(len-1)效率问题其实差距不大，&也就是%的4倍左右的差距；2n的核心原因是hash函数的源码中右移了16位让低位保留高位信息，原本的低位信息不要，那么进行&操作另一个数低位必须全是1，否则没有意义，所以len必须是2n，也就是要尽量把数据分配均匀！**》【讲道理这个解释不错】

**终目的还是为了让哈希后的结果更均匀的分部，减少哈希碰撞，提升hashmap的运行效率**

##### 2，HashMap源码的内部实现逻辑

​		**核心成员变量：**

​			modCount:表示对HashMap结构的修改次数：包括新增，删除，修改；同时也能够检测出HashMap的快速失败机制：

​								什么是fail-fast机制：在使用迭代器对集合对象进行遍历的时候，如果 A 线程正在对集合进行遍历，此时 B 线程对集合																	进行修改（增加、删除、修改），或者 A 线程在遍历过程中对集合进行修改，都会导致 A 线程抛																	出 ConcurrentModificationException 异常。

​			threshold：扩容阈值：使用table（数组）长度length * loadFactor（负载因子）计算得出

​			size：表示当前HashMap中存放的k，v的总数

​		**插入机制：**

​		1,  查询对应的槽位

​		2，判断槽位是否存在对应的值，如果不存在，直接存入table数组，如果存在，
​		3，判断是否为TreeNode（红黑树），插入红黑树节点中，
​		4，如果为普通链表，插入指定链表尾部【同时判断链表长度是否大于8，链表变成红黑树】

​		**链表变成红黑树机制**

​			**[接着上面插入机制的第四步]**如果链表的长度达到固定的：**TREEIFY_THRESHOLD 8** 的时候，此时会将链表转换成红黑树

​		**扩容机制**

​			当当前HashMap中的size >threshold时，进行扩容操作

​			1，获取对应的oldCap【原始槽位数】，和oldThr【原始扩容阈值】

​			2, 计算newCap（oldCap<< 1 即*2），对于oldCap<16, newThr=newCap * threshold，对于oldCap>=16, newThr = oldThr << 1 即乘2

​			3，新建一个newCap大小的【Node<K,V>[] table】

​			4,   遍历原始的table

​				<1>如果指定槽位上有数据，并且不为链表形式，直接通过hash & (newCap-1) 获取新table的槽位，置之
​				<2>如果指定槽位上有数据，并且为TreeNode(红黑树)，通过((TreeNode<K,V>)e).split(this, newTab, j, oldCap);操作【这个还没研究】该操作会将对应的树形结构进行退化，如果退化之后的TreeNode的节点个数小于UNTREEIFY_THRESHOLD【6】会将TreeNode红黑树退化成链表
​				<3>如果指定槽位上有数据，并且为链表形式，此时，旧数组上的数据会通过e.hash & oldCap是否等于0
​       			**如果为0：表示该头节点放到新数组的索引位置等于其旧数组存放的索引位置：（这个算法过于牛批，真的）
​        				验证思路：**
​       				【(h = key.hashCode()) ^ (h >>> 16)】 & 2n次幂
​        				如果e.hash & (oldCap - 1) = e.hash & (2oldCap - 1) 那么一定是e.hash & oldCap == 0
​       			**如果不为0：表示该头节点放到新数组时的索引位置等于其旧数组时的索引位置再加上旧数组长度，**

##### 3，为什么HashMap的加载因子是0.75，链表变成红黑树的阈值是8

​		这个问题，在HashMap的官方文档中有对应的解释

> ```
> * Ideally, under random hashCodes, the frequency of
> * nodes in bins follows a Poisson distribution
> * (http://en.wikipedia.org/wiki/Poisson_distribution) with a
> * parameter of about 0.5 on average for the default resizing
> * threshold of 0.75, although with a large variance because of
> * resizing granularity. Ignoring variance, the expected
> * occurrences of list size k are (exp(-0.5) * pow(0.5, k) /
> * factorial(k)). The first values are:
> *
> * 0:    0.60653066
> * 1:    0.30326533
> * 2:    0.07581633
> * 3:    0.01263606
> * 4:    0.00157952
> * 5:    0.00015795
> * 6:    0.00001316
> * 7:    0.00000094
> * 8:    0.00000006
> * more: less than 1 in ten million
> ```

​		即核心的含义就是，事实上，在随机hashcode码中，节点出现的频率在hash桶中遵循泊松分布，同时给出了桶中元素个数和概率的对照表。

​		即基于0.75的加载因子考虑的情况下（带入到泊松分布算法中），最终计算的结果会发现，对于hash桶中的一个桶出现8个节点的概率会很低。

​		这个也解释了为什么加载因子为0.75的情况下，链表转成红黑树的阈值是8

​		[HashMap的泊松分布](https://blog.csdn.net/weixin_43883685/article/details/109809049)

​		**为什么HashMap的加载因子是：0.75，这个问题官方给定的解释是：**

> ```
> * <p>As a general rule, the default load factor (.75) offers a good
> * tradeoff between time and space costs.  Higher values decrease the
> * space overhead but increase the lookup cost (reflected in most of
> * the operations of the <tt>HashMap</tt> class, including
> * <tt>get</tt> and <tt>put</tt>).  The expected number of entries in
> * the map and its load factor should be taken into account when
> * setting its initial capacity, so as to minimize the number of
> * rehash operations.  If the initial capacity is greater than the
> * maximum number of entries divided by the load factor, no rehash
> * operations will ever occur.
> ```

​		**一般来说，默认的加载系数(0.75)提供了一个很好的选择时间和空间成本的权衡。较高的数值会降低空间开销，但增加查找成本(反映在大多数 `HashMap`类的操作，包括`get` and `put`)…**

​		关于泊松分布可以参考：[泊松分布](http://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html#comment-356111)

#### ConcurrentHashMap

​		1，ConcurrentHashMap对应的key和value都不能为null，这一点和HashMap不相同

​		2，对于ConcurrentHashMap而言，其内部的数据结构和HashMap一致：数组+链表+红黑树

​		3，对于HashMap而言，是有对应fail-fast机制，但是对于ConcurrentHashMap而言，如何解决？

​				解答：ConcurrentHashMap是不存在对应的fail-fast机制，相反，它是fail-safe（即：安全失败），

​				在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。

##### 1，ConcurrentHashMap的size获取

​		先尝试通过cas更新baseCount计数

​		如果多线程竞争激烈，某些线程 CAS 失败，那就 CAS 尝试将 cellsBusy 置 1，成功则可以把 baseCount 变化的次数暂存到一个数组 			counterCells 里，后续数组 counterCells 的值会加到 baseCount 中。
​		如果 cellsBusy 置 1 失败又会反复进行 CAS baseCount 和 CAS counterCells 数组。

​		**重点：**

​				1，首先，ConcurrentHashMap中的baseCount存放的是未发生cas锁失败的记录总数

​				2，当尝试cas修改baseCount失败之后，实际上会将当前失败的次数存储在countCells数组中，所以CountCells存放 的都是value为1的CounterCell对象，而这些对象是因为在CAS更新baseCounter值时，由于高并发而导致失败，最终将值保存到CounterCell中，放到counterCells里。这也就是为什么sumCount()中需要遍历counterCells数组，sum累加CounterCell.value值了。

##### 2，ConcurrentHashMap的并发协助扩容机制

​		[ConcurrentHashMap的并发协助扩容机制](https://www.cnblogs.com/FondWang/p/12142149.html)

### 数据库

#### Mysql中的B树和B+树的区别

​		[一文彻底搞懂MySQL基础：B树和B+树的区别](https://blog.csdn.net/a519640026/article/details/106940115/)

​		B树具有以下的特点：

​				所有键值分布在整颗树中（索引值和具体data都在每个节点里）；

​				任何一个关键字出现且只出现在一个结点中；

​				搜索有可能在非叶子结点结束（最好情况O(1)就能找到数据）；

​				在关键字全集内做一次查找,性能逼近二分查找；

​		B+树是B树的变体，也是一种多路搜索树，它于B树的不同之处在于：

​				所有关键字存储在叶子节点出现,内部节点(非叶子节点并不存储真正的 data)

​				为所有叶子结点增加了一个链指针

#### Mysql的主从复制，以及Mysql的binlog主从复制延迟的原因

Mysql的binlog日志

设置binlog
log_bin=mysql_bin
log_bin_index=mysql_bin.index
其内部会在指定的binlog目录中生成对应的mysql_bin.xxxx01编号对应的binlog日志文件

sync_binlog=10
sync_binlog=0，当事务提交之后，MySQL不做fsync之类的磁盘同步指令刷新binlog_cache中的信息到磁盘，而让Filesystem自行决定什么时候来做同步，或者cache满了之后才同步到磁盘。
sync_binlog=n，当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘。 



Mysql的binlog日志类型

**STATEMENT：基于SQL语句的复制(statement-based replication, SBR)**
             STATEMENT是基于sql语句级别的binlog,每一条修改数据的sql都会被保存到binlog里

**ROW:基于行的复制(row-based replication, RBR)**
             ROW是基于行级别的,他会记录每一行记录的变化,就是将每一行的修改都记录到binlog里面,记录的非常详细，但sql语句并没有在binlog里,在replication里面也不会因为存储过程触发器等造成Master-Slave数据不一致的问题,但是有个致命的缺点日志量比较大.由于要记录每一行的数据变化,当执行update语句后面不加where条件的时候或alter table的时候,产生的日志量是相当的大。

**MIXED:混合式的模式**



**statement模式的优缺点：**
1,binlog日志文件小，并且可读性好，binlog中包含了所有数据库修改信息，可以据此来审核数据库的安全等情况binlog可以用于实时的还原，而不仅仅用于复制，
2,核心缺点是并不能记录所有的Update语句，比如UUID(),FOUND_ROWS() 其中INSERT...UPDATE会产生更多的行级锁



**row模式的优缺点：**

优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以row的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题.

缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。



**Mysql的主从复制的原理**

1，MySQL主从复制的原理是基于binlog日志

mysql的主从复制都是单线程的操作，主库对所有DDL和 DML产生binlog，binlog是顺序写，所以效率很高，slave的Slave_IO_Running线程到主库取日志，效率很比较高，下一步， 问题来了，slave的Slave_SQL_Running线程将主库的DDL和DML操作在slave实施。DML和DDL的IO操作是随即的，不是顺 序的，成本高很多，还可能可slave上的其他查询产生lock争用，由于Slave_SQL_Running也是单线程的，所以一个DDL卡主了，需要 执行10分钟，那么所有之后的DDL会等待这个DDL执行完才会继续执行，这就导致了延时。有朋友会问：“主库上那个相同的DDL也需要执行10分，为什 么slave会延时？”，答案是master可以并发，Slave_SQL_Running线程却不可以。

**Mysql主从数据库同步延迟是怎么产生的？**

当主库的TPS并发较高时，产生的DDL数量超过slave一个sql线程所能承受的范围，那么延时就产生了，当然还有就是可能与slave的大型query语句产生了锁等待。读写锁的竞争问题，都会导致对应的同步延迟问题。

**Mysql数据库主从同步延迟解决方案**

最简单的减少slave同步延时的方案就是在架构上做优化，尽量让主库的DDL快速执行。还有就是主库是写，对数据安全性较高，比如 **sync_binlog=1，innodb_flush_log_at_trx_commit = 1** 之类的设置，而slave则不需要这么高的数据安全，完全可以讲sync_binlog设置为0或者关闭binlog，innodb_flushlog也 可以设置为0来提高sql的执行效率。另外就是使用比主库更好的硬件设备作为slave。

mysql-5.6.3已经支持了多线程的主从复制。



#### Mysql如何分页查询获取最后几页的数据

1， 直接使用mysql 提供的limit n， m语法进行分页：SELECT * FROM 表名称 LIMIT M,N
这种写法只适用于数据量比较少的情况，缺点就是在查询表的某位页数的时候，实际上会导致全表扫描，从而性能很低

2，使用order by 字段 ，并且order by的字段必须设置索引

3，根据对应的主键或者唯一索引，利用索引进行查询：（必须要保证对应的主键id或者唯一id连续）
SELECT * FROM 表名称 WHERE id_pk > (pageNum*10) LIMIT M

4，把limit偏移量限制低于某个数。。超过这个数等于没数据，我记得alibaba的dba说过他们是这样做的（优化页面性能）



#### Mysql中的MVCC和间隙锁的区别？

**MVCC是一种用来解决读-写冲突的无锁并发控制**

**非唯一索引下的范围查询会触发对应的间隙锁**

对一个未唯一索引的更新操作，会引发另一个事务无法插入当前索引

[[Innodb中的事务隔离级别和锁的关系](https://tech.meituan.com/2014/08/20/innodb-lock.html)](https://tech.meituan.com/2014/08/20/innodb-lock.html)

Mysql中的快照读和当前读的区别：

快照读：（就是select *）

​		select * from table ….;

当前读：（特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁）

​		select * from table where ? lock in share mode;

​		select * from table where ? for update;
​		insert;
​		update ;
​		delete;

事务的隔离级别实际上都是定义了当前读的级别，MySQL为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得select不用加锁。而update、insert这些“当前读”，就需要另外的模块来解决了。

**为了解决当前读中的幻读问题，Mysql事务使用了Next-key锁**



#### mysqlbinlog日志和redolog日志写入机制

通过两阶段提交的策略：

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2
这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内
存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的
一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo
log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状
态，更新完成。



### 网络

#### TCP三次握手/四次挥手机制

[TCP三次握手详解及释放连接过程](https://www.cnblogs.com/kaleidoscope/p/9701117.html)

[知乎讲解：TCP](https://www.zhihu.com/question/271701044)

### Redis

#### redis的数据结构

​		redis的内部数据结构分为：简单动态字符（SDS），字典（Dict），链表，压缩链表，跳跃表（skipList，zset，有序集合对象内部的数据结果），整形集合

​		[Redis内部分析](https://www.cnblogs.com/hunternet/p/9957913.html)【个人认为老哥的这个一些列博客讲的很是牛批】

redis常用的数据结构有哪些，内部是使用的底层数据结构分别是哪些？

​		字符串String: 	SDS, 整形集合（int）

​		哈希（hash）：字典，压缩链表

​		列表（list）：链表，压缩链表

​		集合（Set）：字典，整形集合（int）

​		有序列表: 压缩链表，跳跃表

#### redis的持久化机制

[**Redis提供的持久化机制**](https://www.cnblogs.com/xingzc/p/5988080.html)

​		RDB：

​				RDB 持久化方式能够在指定的时间间隔能对你的数据进行快照（snapshotting）存储，将内存中的数据不断写入二进制文件中，默认文件dump.rdb，可配置Redis在n秒内如果超过m个key被修改就自动保存快照。（性能高，但是可能会出现数据丢失）
例
save 900 1 #900秒内如果超过1个key被修改，则发起快照保存。
save 300 10 #300秒内如果超过10个key被修改，则快照保存。

RDB持久化只会周期性的保存数据，在未触发下一次存储时服务宕机，就会丢失增量数据。当数据量较大的情况下，fork子进程这个操作很消耗cpu，可能会发生长达秒级别的阻塞情况。

​		AOF：

​				AOF（Append-only file） 持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大。（类似于MySql的日志方式，记录每次更新的日志）（性能低，但是数据完整）



#### redis的事务

redis中的事务
事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。（这个redis似乎是能够去影响的）
事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。

Redis的事务的本质是通过MULTI, EXEC, WATCH等一组命令的集合

multi:开启一个事务

exec：表示执行事务

watch：命令是 watch keys 【监视指定的key。如果exec执行之前已经监视了某些key，如果key被另一个客户端修改对应的值，则execu对应的事务队列讲不会执行】

DISCARD：清空对应的事务中的commond队列，恢复连接状态（注意这个命令必须再multi之后执行【exec之前】），并且也能够清除对应的watch
了解了Redis事务机制后，我们继续看如何使用命令实现事务。



**redis的事务就是一致性，顺序性，排他性的执行一个队列中的命令**

**redis事务中出错怎么办？**

常见的错误：
	1，命令可能存在语法错误，进入队列的命令有误，比如参数数量错误，错误的命令名称
	2，执行EXEC运行时候时出错，比如给一个list类型的变量 执行incr + 1，这样的命令语法上没问题，只有在运行的时候才能发行

对于第一种错误，客户端会在EXEC调用之前检测， 通过检查排队命令的状态回复，如果命令使用QUEUED进行响应，则它已正确排队；否则Redis将返回错误。
对于第二种错误，服务端会记住在累积命令期间发生的错误，当EXEC命令调用时，将拒绝执行事务，并返回这些错误，同时自动清除命令队列。即使事务中的某些命令执行失败，其他命令仍会被正常执行。（包括出错命令之后的命令）
127.0.0.1:6379> multi
OK
127.0.0.1:6379> incr zhoucg
QUEUED
127.0.0.1:6379> incr y
QUEUED
127.0.0.1:6379> exec
1) (error) WRONGTYPE Operation against a key holding the wrong kind of value
2) (integer) 5
127.0.0.1:6379> 

**为什么redis的事务不支持会滚机制**

这个有点甩锅操作？

事实上Redis命令在事务执行时可能会失败，但仍会继续执行剩余命令而不是Rollback（事务回滚）。如果你使用过关系数据库，这种情况可能会让你感到很奇怪。然而针对这种情况具备很好的解释：

Redis命令可能会执行失败，仅仅是由于错误的语法被调用（命令排队时检测不出来的错误），或者使用错误的数据类型操作某个Key： 这意味着，实际上失败的命令都是编程错误造成的，都是开发中能够被检测出来的，生产环境中不应该存在。（这番话，彻底甩锅，“都是你们自己编程错误，与我们无关”。）
由于不必支持Rollback,Redis内部简洁并且更加高效。
“如果错误就是发生了呢？”这是一个反对Redis观点的争论。然而应该指出的是，通常情况下，回滚并不能挽救编程错误。鉴于没有人能够挽救程序员的错误，并且Redis命令失败所需的错误类型不太可能进入生产环境，所以我们选择了不支持错误回滚（Rollback）这种更简单快捷的方法。

**redis的事务是否具有隔离性**

Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的

#### redis的管道（pipline）

[**Redis 管道技术**](https://www.liangzl.com/get-article-detail-133400.html)

redis是一种基于客户端/服务断模式以及请求响应协议的/TCP服务，
客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。
服务端处理命令，并将结果返回给客户端。

redis的管道技术是能够一次性的发送多个redis执行命令，并且最终一次性的读取所有服务端的响应
一次请求/响应服务器能实现处理新的请求即使旧的请求还未被响应。这样就可以将多个命令发送到服务器，而不用等待回复，最后在一个步骤中读取该答复。这就是管道（pipelining）

减少了传统型TCP请求响应的socker上的响应等待，

#### redis的缓存雪崩

缓存雪崩指得是缓存突然一次性大面积的失效，导致高并发都访问数据库，从而导致数据库的压力过大，从而导致崩掉

解决策略：
1，适当的调整合适的内存淘汰策略，避免大面积的缓存再同一个时间失效
2，增加redis集群的高可用

### Kafka

Kafka是一个分布式得流式处理平台（a distributed streaming platform）
1，它允许您发布和订阅流记录。在这方面,它类似于一个消息队列或企业消息传递系统。
2，它允许您以容错的方式存储记录流。
3，它允许您在记录流发生时处理它们。

#### kafka的主要组成部分

**生产者（Producer）：**

​	发送消息得一方，生产者负责创建消息，然后将其投递到Kafka中（投递到kafka的指定topic中）

**消费者（Consumer）**

​	消费者连接Kafka上并接收消息，进而进行响应得业务逻辑处理（接收指定topic中的消息）

**节点（Broker）**

​	表示Kafka集群中得单个节点

**主题（Topic）**

​	kafka中得消息是以主题为单位进行归类，生产者负责将消息发送到特定得主题中，而消费者负责订阅主题并进	行消费

**分区（Partition）**

​	**一个Topic可以细分多个分区，一个分区只属于单个主题，同一个主题下得不同分区包含得消息是不同得，分		区在存储层面是一个可追加得日志（Log）文件**

**分段(Segment)**

​	分段是kafka对于分区Partition消息的具体体现（就是一个具体的文件存储），在kafka的log.dir目录下，会根据	对应的topic和partition生成对应的目录
​	topic-partition

**分区副本（Partition Replica）**

​	1，通过增加分区副本来提高容灾能力

​	2，同一个分区的不同副本中保存的式同样的消息（在同一个时刻，副本之间并非完全一样，由于ISR副本复制	的时间延迟问题导致）

​	3，副本之间是**一主多从**得关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本得消息	同步。

​	4，每个服务器作为其某个分区得leader和其他分区得follower，因此集群内得负载都是平衡得

​	5，对于分区得副本而言，leader对应得分区和follower是分布在不同得broker中（为了提升容灾能力）	

​	6，分区副本包含leader副本+follower副本；对于创建分区副本为3得数据而言，其内部是含有一个leader副本	和两个follower副本

**ISR(In-Sync-Replicas)**

​	所有leader副本保持一定程度同步得副本（包括leader副本在内）组成ISR

**offset**

​	每个partition都由一系列有序的，不可变的消息组成，这些消息被连续的追加到partition中，partition中的每个消息都有一个连续的序列号叫做offset，用于partition唯一标识一条消息

#### kafka得消息投递机制

​	每一个消息在被发送到broker之前，都会根据**分区规则**选择存储到哪个具体得分区

​	分区规则的配置，在

#### 核心命令

##### kafka-topics.sh

​		1，**创建kafka的topic**

​			**./kafka-topics.sh --create --zookeeper 192.168.129.128:2181/kafka-group/default --replication-factor 1 --partitions 1 --topic zhoucg-topic-wl**
​			--zookeeper 192.168.129.128:2181/kafka-group/default 配置的是kafka连接zookeeper的路径（老版本的写法），如果kafka的server.properties配置了zookeeper的具体node节点信息，则命令后面也要相同，不然报错
​			--replication-factor 副本策略，如果配置为3，对应的每一个分区（partition），总共一个leader副本，两个follower副本
​			--partitions 对应topic（主题）的分区个数
​			--topic 对应创建主题的名称（Topic 名称中一定不要同时出现下划线 (’_’) 和小数点 (’.’)）

​		**2，展示创建topic（主题）的具体信息**

		./kafka-topics.sh --zookeeper 192.168.129.128:2181/kafka-group/default --describe --topic
​			--zookeeper 配置对应kafka的zookeeper参数
​			--describe 订阅主题
​			Topic:zhoucg-topic      PartitionCount:1        ReplicationFactor:1     Configs:
​      			Topic: zhoucg-topic     Partition: 0    Leader: 100001  Replicas: 100001        Isr: 100001
​			Topic:zhoucg-topic-wl   PartitionCount:1        ReplicationFactor:1     Configs:
​     			Topic: zhoucg-topic-wl  Partition: 0    Leader: 100001  Replicas: 100001        Isr: 100001

​		**3，查找topic（主题）列表**

		./kafka-topics.sh --zookeeper 192.168.129.128:2181/kafka-group/default --list
​		--zookeeper kafka对应的zookeeper配置信息
​		--list 对应的topic列表信息展示

​		**4，修改Topic对应的分区和副本信息**

		**./kafka-topics.sh --alter --zookeeper 192.168.129.128:2181/kafka-group/default --topic zhoucg-topic --partitions 3
​		注意：kafka指定的topic对应的副本数据不能进行修改，只能修改分区数据**
​			--alter 修改命令
​			--zookeeper 参数
​			--topic topic名称
​			--partitions 修改对应的分区数据

​		**5，删除指定Topic**

		./kafka-topics.sh --delete --zookeeper 192.168.129.128:2181/kafka-group/default --topic zhoucg-topic-wl 

​		--delete 删除模式

​		**6，Kafka生产者Console**

		./kafka-console-producer.sh --broker-list 192.168.129.128:9192 --topic zhoucg-topic

​		**7，Kafka消费者Console**

		./kafka-console-consumer.sh --topic zhoucg-topic --from-beginning --zookeeper 192.168.129.128:2181/kafka-group/default

##### kafka-consumer-groups.sh

​		**1，查询当前Kafka集群中的消费组消息**

		./kafka-consumer-groups.sh --bootstrap-server 192.168.129.128:9192 --list
​		--boostrap-server：对于kafka的集群配置
​		--list: 查询列表

​		**2，查询消费组对应的Topic的信息**

		/kafka-consumer-groups.sh --bootstrap-server 192.168.129.128:9192 --group groupId1 --describe

​				--group:订阅的消费组名称
​				返回示例：
​				TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID           HOST            CLIENT-ID
​				topic-18        0          0               0               0               consumer-2-034e48da-9422-4502-851d-511e3cc017a0 /192.168.129.1  				consumer-2
​				topic-4         0          24671           24708           37              -                                               -               -
​				topic-3         0          4305            4327            22              -                                               -               -
​		**其中：存在对应的HOST，CLIENT-ID信息的实际上是标签当前的消费组正在订阅指定的topic**
​		**不存在对应的HOST,CLIENT-ID信息标识当前的消费组已经关闭订阅指定的topic信息**

### Dubbo

### Zookeeper

### Netty

### Elasticsearch

### Spring

#### Spring的Bean的生命周期

[请别再问Spring Bean的生命周期了！](https://www.jianshu.com/p/1dec08d290c1)

1, Spring IOC会通过xml或者属性注解的方式将对应的bean的配置转换成BeanDefinition

2，实例化 Instantiation

3，属性赋值Populate

4，初始化Initialization

5，销毁 Destruction

#### Spring的循环依赖问题

1，Spring是无法解决构造器的循环依赖

2，Spring是无法解决设置对象类型为prototype的field属性输入的循环依赖

3，Spring能够解决对象类型为单例的field属性注入循环依赖

**Spring是如何解决单例类型Field属性注入的循环依赖**
Spring创建bean的原则是不等bean创建完成就会将创建bean的ObjectFactory提早曝光（这一步实际上是在实例化之后，属性赋值之前操作），**存放到对应的spring内部的第三级缓存singletonFactories**中。
**那么问题来了，spring中的三级缓存分别存放的是什么？**
singletonObjects：存放所有singletonBean的实例
earlySingletonObjects：存放早期创建的singletonBean，这个Bean还没有完成依赖注入
singletonFactories： singletonBean的生产工厂

#### BeanFactory和ApplicationContext之间的区别

1，首先，BeanFactory是Spring内部IOC的核心实现，他只是提供对应的实例化对象和拿对象的功能，

2，ApplicationContext更像是容器,.应用上下文，继承BeanFactory接口，它是Spring的一个更高级的容器，提供了更多的有用的功能；
国际化，访问资源，Event事件机制，AOP拦截器
其中：
**BeanFactory在启动的时候不会去实例化Bean，只有从容器中拿Bean的时候才会去实例化；
ApplicationContext在启动的时候就把所有的Bean全部实例化了。它还可以为Bean配置lazy-init=true来让Bean延迟实例化；**

#### Spring中BeanPostProcessor和BeanFactoryPostProcessor的区别

Spring中BeanFactoryPostProcessor 和 BeanPostProcessor的使用
在去理清这两个接口的时候之前，需要先关注一下Spring在什么时候是实例化对象的？
一个原理：
**Spring容器初始化bean大致过程是： 定义bean标签[1] --> 将bean标签解析成BeanDefinition[2]-->调用构造方法实例化[3]--->属性值的依赖注入[4] ->初始化 ->
两种处理的方式：
BeanFactoryPostProcessor 作用于第二步和第三步之间，而 BeanPostProcessor 作用于第三步之后的初始化的前后。**

### Spring Boot

### Spring Cloud

#### Eureka内部服务原理

Eureka的内部高可用机制

[Eureka原理分析](https://zhuanlan.zhihu.com/p/88385121)

##### Eureka服务节点的自我保护机制：

Eureka的自我保护机制是为了防止，因网络问题导致集群服务节点大量剔除本来服务可用的服务实例，自我保护机制主要在Eureka Client和Eureka Server之间存在网络分区的情况下发挥保护作用，在服务器端和客户端都有对应实现。假设在某种特定的情况下（如网络故障）, Eureka Client和Eureka Server无法进行通信，此时Eureka Client无法向Eureka Server发起注册和续约请求，Eureka Server中就可能因注册表中的服务实例租约出现大量过期而面临被剔除的危险，然而此时的Eureka Client可能是处于健康状态的（可接受服务访问），如果直接将注册表中大量过期的服务实例租约剔除显然是不合理的，自我保护机制提高了eureka的服务可用性。

##### Eureka集群原理：

节点公平，弱一致性机制
Eureka的集群各个节点之间没有主从之分，遵从公平的原则，



##### Eureka的服务发现原理：

eureka server可以集群部署，多个节点之间会进行（异步方式）数据同步，保证数据最终一致性，Eureka Server作为一个开箱即用的服务注册中心，提供的功能包括：服务注册、接收服务心跳、服务剔除、服务下线等。需要注意的是，Eureka Server同时也是一个Eureka Client，在不禁止Eureka Server的客户端行为时，它会向它配置文件中的其他Eureka Server进行拉取注册表、服务注册和发送心跳等操作。

##### Eureka保证AP:

Eureka Server 各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而 Eureka Client 在向某个 Eureka 注册时，如果发现连接失败，则会自动切换至其它节点。只要有一台 Eureka Server 还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。

##### Eureka和Zookeeper之间的区别：

Eureka保证的是:AP:

​		Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障。

Zookeeper保证的是CP:

​		当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。

##### Eureka Consumer的拉取机制

Eureka consumer服务信息的拉取分为**全量式拉取**和**增量式拉取**，eureka consumer启动时进行全量拉取，运行过程中由定时任务进行增量式拉取，如果网络出现异常，可能导致先拉取的数据被旧数据覆盖（比如上一次拉取线程获取结果较慢，数据已更新情况下使用返回结果再次更新，导致数据版本落后），产生脏数据。对此，eureka通过类型AtomicLong的fetchRegistryGeneration对数据版本进行跟踪，版本不一致则表示此次拉取到的数据已过期。

fetchRegistryGeneration过程是在拉取数据之前，执行fetchRegistryGeneration.get获取当前版本号，获取到数据之后，通过fetchRegistryGeneration.compareAndSet来判断当前版本号是否已更新。
注意：如果增量式更新出现意外，会再次进行一次全量拉取更新。

##### Eureka Server 的伸缩性

Eureka Server是怎么知道有多少Peer的呢？Eureka Server在启动后会调用EurekaClientConfig.getEurekaServerServiceUrls来获取所有的Peer节点，并且会定期更新。定期更新频率可以通过eureka.server.peerEurekaNodesUpdateIntervalMs配置。

这个方法的默认实现是从配置文件读取，所以如果Eureka Server节点相对固定的话，可以通过在配置文件中配置来实现。如果希望能更灵活的控制Eureka Server节点，比如动态扩容/缩容，那么可以override getEurekaServerServiceUrls方法，提供自己的实现，比如我们的项目中会通过数据库读取Eureka Server列表。

eureka server启动时把自己当做是Service Consumer从其它Peer Eureka获取所有服务的注册信息。然后对每个服务信息，在自己这里执行Register，isReplication=true，从而完成初始化。

##### Eureka的不足之处

eureka有哪些不足： eureka consumer本身有缓存，服务状态更新滞后，最常见的状况就是，服务下线了但是服务消费者还未及时感知，此时调用到已下线服务会导致请求失败，只能依靠consumer端的容错机制来保证。



### 微服务架构

#### 分布式事务

[分布式事务架构](http://www.tianshouzhi.com/api/tutorials/distributed_transaction/383)

分布式事务类型：

​			跨库事务

​			分库分表

​			微服务（SOA）：TCC两阶段提交：柔性分布式事务解决

DTP分布式事务模型（Distributed Transaction Processing: Reference Model）

 **应用程序(Application Program ，简称AP)：**用于定义事务边界(即定义事务的开始和结束)，并且在事务边界内对资源进行操作。

 **资源管理器(Resource Manager，简称RM)：**如数据库、文件系统等，并提供访问资源的方式。

 **事务管理器(Transaction Manager ，简称TM)：**负责分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚等。

 **通信资源管理器(Communication Resource Manager，简称CRM)：**控制一个TM域(TM domain)内或者跨TM域的分布式应用之间的通信。

  **通信协议(Communication Protocol，简称CP)：**提供CRM提供的分布式应用节点之间的底层通信服务。



XA规范：

​		XA规范主要定义了RM和TM之间的协议（分布式事务模型DTP中定义了常用分布式事务模型的节点功能）

​		XA规范除了定义的RM-TM交互的接口(XA Interface)之外，还对两阶段提交协议进行了优化。 一些读者可能会误认为两阶段提交协议是在XA规范中提出来的。事实上： 两阶段协议(two-phase commit)是在OSI TP标准中提出的；在DTP参考模型(<<Distributed Transaction Processing: Reference Model>>)中，指定了全局事务的提交要使用two-phase commit协议；而XA规范(<< Distributed Transaction Processing: The XA Specification>>)只是定义了两阶段提交协议中需要使用到的接口，也就是上述提到的RM-TM交互的接口，因为两阶段提交过程中的参与方，只有TM和RMs。 

**XA规范对两阶段（2PC）提交进行了优化**：

两阶段的问题，和三阶段对应的优化点（2PC）（3PC）

两阶段提交（2PC）：prepare commit/cancel

三阶段提交（3PC）：canCommit/preCommit/doCommit



CAP理论：分布式系统

BASE:	BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。



在DTP模型中，Mysql只是属于RM（资源管理器），而一个完整的分布式事务中，一般会存在多个RM，由事务管理器TM来统一进行协调。因此，这里所说的mysql对XA分布式事务的支持，一般指的是单台mysql实例如何执行自己的事务分支。



**java事务API(JTA:Java Transaction API)**

[JTA规范](http://www.tianshouzhi.com/api/tutorials/distributed_transaction/385)

​			某种程度上，可以认为JTA规范事XA规范的java版，其把XA规范中规定的DTP模型交互接口抽象成Java接口中的方法，并规定每个方法要实现什么样的功能

![](image/JTA.png)

**事务管理器（TM）:**

​		 处于图中最为核心的位置，其他的事务参与者都是与事务管理器进行交互。事务管理器提供事务声明，事务资源管理，同步，事务上下文传播等功能。JTA规范定义了事务管理器与其他事务参与者交互的接口，而JTS规范定义了事务管理器的实现要求，因此我们看到事务管理器底层是基于JTS的。

**应用管理器：（application server)**

​		顾名思义，是应用程序运行的容器。JTA规范规定，事务管理器的功能应该由application server提供，如上图中的EJB Server。一些常见的其他web容器，如：jboss、weblogic、websphere等，都可以作为application server，这些web容器都实现了JTA规范。特别需要注意的是，并不是所有的web容器都实现了JTA规范，如tomcat并没有实现JTA规范，因此并不能提供事务管理器的功能。

**应用程序(application)：**

   简单来说，就是我们自己编写的应用，部署到了实现了JTA规范的application server中，之后我们就可以我们JTA规范中定义的UserTransaction类来声明一个分布式事务。通常情况下，application server为了简化开发者的工作量，并不一定要求开发者使用UserTransaction来声明一个事务，开发者可以在需要使用分布式事务的方法上添加一个注解，就像spring的声明式事务一样，来声明一个分布式事务。

  特别需要注意的是，JTA规范规定事务管理器的功能由application server提供。但是如果我们的应用不是一个web应用，而是一个本地应用，不需要被部署到application server中，无法使用application server提供的事务管理器功能。又或者我们使用的web容器并没有事务管理器的功能，如tomcat。对于这些情况，我们可以直接使用一些第三方的事务管理器类库，如JOTM和Atomikos。将事务管理器直接整合进应用中，不再依赖于application server。

**资源管理器(resource manager)：**

  理论上任何可以存储数据的软件，都可以认为是资源管理器RM。最典型的RM就是关系型数据库了，如mysql，另外一种比较常见的资源管理器是消息中间件，如ActiveMQ、RabbitMQ等， 这些都是真正的资源管理器。  

  事实上，将资源管理器(resource manager)称为资源适配器(resource adapter)似乎更为合适。因为在java程序中，我们都是通过client来于RM进行交互的，例如：我们通过mysql-connector-java-x.x.x.jar驱动包，获取Conn、执行sql，与mysql服务端进行通信；通过ActiveMQ、RabbitMQ等的客户端，来发送消息等。

  正常情况下，一个数据库驱动供应商只需要实现JDBC规范即可，一个消息中间件供应商只需要实现JMS规范即可。 而引入了分布式事务的概念后，DB、MQ等在DTP模型中的作用都是RM，二者是等价的，需要由TM统一进行协调。

   **为此，JTA规范定义了一个XAResource接口，其定义RM必须要提供给TM调用的一些方法。之后，不管这个RM是DB，还是MQ，TM并不关心，因为其操作的是XAResource接口。而其他规范(如JDBC、JMS)的实现者，同时也对此接口进行实现。如MysqlXAConnection，就实现了XAResource接口。**

#### 柔性事务 ：TCC两阶段补偿型

[柔性事务 ：TCC两阶段补偿型](http://www.tianshouzhi.com/api/tutorials/distributed_transaction/388)

**TCC（Try-Confirm-Cancel）的作用主要用来解决跨服务调用场景下的分布式事务问题**

TCC是Try-Confirm-Cancel的简称:

**Try阶段：**

  完成所有业务检查（一致性），预留业务资源(准隔离性)

  回顾上面航班预定案例的阶段1，机票就是业务资源，所有的资源提供者(航空公司)预留都成功，try阶段才算成功

**Confirm阶段：**

  确认执行业务操作，不做任何业务检查， 只使用Try阶段预留的业务资源。回顾上面航班预定案例的阶段2，美团APP确认两个航空公司机票都预留成功，因此向两个航空公司分别发送确认购买的请求。、

**Cancel阶段：**

   取消Try阶段预留的业务资源。回顾上面航班预定案例的阶段2，如果某个业务方的业务资源没有预留成功，则取消所有业务资源预留请求。 



TCC两阶段提交与XA两阶段提交的区别是：

​		**XA是资源层面的分布式事务，强一致性，在两阶段提交的整个过程中，一直会持有资源的锁。**	

​		**TCC是业务层面的分布式事务，最终一致性，不会一直持有资源的锁。**



为什么说TCC是补偿型事务：

​		"补偿是一个独立的支持ACID特性的本地事务，用于在逻辑上取消服务提供者上一个ACID事务造成的影响，对于一个长事务(long-running transaction)，与其实现一个巨大的分布式ACID事务，不如使用基于补偿性的方案，把每一次服务调用当做一个较短的本地ACID事务来处理，执行完就立即提交”。

   在这里，笔者理解为confirm和cancel就是补偿事务，用于取消try阶段本地事务造成的影响。因为第一阶段try只是预留资源，之后必须要明确的告诉服务提供者，这个资源你到底要不要，对应第二阶段的confirm/cancel。

  提示：读者现在应该明白为什么把TCC叫做两阶段补偿性事务了，提交过程分为2个阶段，第二阶段的confirm/cancel执行的事务属于补偿事务。 

#### 服务治理

**1，什么是服务治理，服务治理包含哪几个方面**

微服务架构下的服务治理：

1，注册中心
2，服务注册和发现
3，配置中心
4，链路监控
5，服务降级，熔断策略
6，服务负载均衡
7，服务容错机制
8，服务限流

**2，注册中心的原理**

参考微服务中的Eureka

**3，为什么配置中心也是服务治理的一部分**

分布式配置服务作为微服务架构必不可缺少的能力，自然是服务治理的一部分

**4，链路监控(链路监控的原理，并发调用多个下游服务怎么区分这些请求)**

链路监控主要分为以下几个模块，agent代理（数据采集，skywalking通过jdk的字节码增强器**java.lang.instrument包提供的字节码增强功能来实现的**），由agent代理回去的数据推送到指标存储，然后再进行链路分析

**spring boot Admin
zipkin
Metrics
SkyWalking
**

吞吐量，根据拓扑可计算相应组件、平台、物理设备的实时吞吐量。
响应时间，包括整体调用的响应时间和各个服务的响应时间等。
错误记录，根据服务返回统计单位时间异常次数。


一般的链路监控主要包括以下几个方面：
**埋点与生成日志：埋点日志通常要包含以下内容traceId、spanId、调用的开始时间，协议类型、调用方ip和端口，请求的服务名、调用耗时，调用结果，异常信息等，同时预留可扩展字段，为下一步扩展做准备；**

**Span: 基本工作单元，一次链路调用（可以是RPC，DB等没有特定的限制）创建一个span，通过一个64位ID标识它，uuid较为方便，span中还有其他的数据，例如描述信息，时间戳，key-value对的（Annotation）tag信息，parent_id等,其中parent-id可以表示span调用链路来源。**

**Trace:类似于 树结构的Span集合，表示一次完整的跟踪，从请求到服务器开始，服务器返回response结束，跟踪每次rpc调用的耗时，存在唯一标识trace_id。比如：你运行的分布式大数据存储一次Trace就由你的一次请求组成。**


请求调用示例

当用户发起一个请求时，首先到达前端A服务，然后分别对B服务和C服务进行RPC调用；
B服务处理完给A做出响应，但是C服务还需要和后端的D服务和E服务交互之后再返还给A服务，最后由A服务来响应用户的请求；

调用过程追踪

**请求到来生成一个全局TraceID，通过TraceID可以串联起整个调用链，一个TraceID代表一次请求。
除了TraceID外，还需要SpanID用于记录调用父子关系。每个服务会记录下parent id和span id，通过他们可以组织一次完整调用链的父子关系。
一个没有parent id的span成为root span，可以看成调用链入口。
所有这些ID可用全局唯一的64位整数表示；
整个调用过程中每个请求都要透传TraceID和SpanID。
每个服务将该次请求附带的TraceID和附带的SpanID作为parent id记录下，并且将自己生成的SpanID也记录下。
要查看某次完整的调用则 只要根据TraceID查出所有调用记录，然后通过parent id和span id组织起整个调用父子关系。**

### IO模型

[Linux IO模式及 select、poll、epoll详解](https://segmentfault.com/a/1190000003063859)[总结的确实很到位]

#### 常见的IO模型

阻塞 I/O（blocking IO），非阻塞 I/O（nonblocking IO）， I/O 多路复用（ IO multiplexing），信号驱动（signal driven IO）， 异步 I/O（asynchronous IO）

#### 用户空间和内核空间

[linux用户态和内核态](https://blog.csdn.net/zcswl7961/article/details/109255583)

用户空间（也叫做用户态）内核空间（也叫做内核态）
内核从本质上看是一种软件——控制计算机的硬件资源，并提供上层应用程序运行的环境。用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括CPU资源、存储资源、I/O资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用。

什么叫做线程切换，线程的上下文切换做了什么

#### 什么叫做进程切换

为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。

任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的

1. 保存处理机上下文，包括程序计数器和其他寄存器。

2. 更新PCB信息。

3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。

4. 选择另一个进程执行，并更新其PCB。

5. 更新内存管理的数据结构。

6. 恢复处理机上下文。



#### IO多路复用

**所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。**

这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态（fd）(对应空管塔里面的Fight progress strip槽)来同时管理多个I/O流
**多路网络连接复用一个IO线程**

**select**：

​		`int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);`

第一个参数表示监听的最大fd的个数+1（最大fd表示的是后面三个fd_set的最大设置值，加1因为fd本身是会生成一个fd）
第二，三，四参数表示select监听的事件类型，read，写，或者异常的fd

最后一个参数表示设置select的阻塞类型，
如果设置为null，表示非阻塞，select立即返回
或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。

select的缺点：
		一个线程能够监听的fs的数量有限，1024个
		对于监听到的对应的fs就绪之后，select仅仅是返回，并不会具体的告诉你是哪个socket数据就绪，需要轮询进行判断对应的fd的就绪状态
		select 不是线程安全的，如果你把一个socket加入到select, 然后突然另外一个线程发现，这个socket不用，要收回。对不起，这个select 不支持的，如果你丧心病狂的竟然关掉这个sock, select的标准行为是。。呃。。不可预测的， 这个可是写在文档中的哦.



**poll**

​		`int poll (struct pollfd *fds, unsigned int nfds, int timeout);`

不同于select使用三个位图来表示fdset的方式，poll使用一个pollfd的指针实现

```
struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。



从上面看，select和poll都需要在返回后，`通过遍历文件描述符来获取已经就绪的socket`。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

## 

**epoll**

epoll使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

```
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

**1. int epoll_create(int size);**
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，`参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议`。
当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。

**2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event \*event)；**
函数是对指定描述符fd执行op操作。
\- epfd：是epoll_create()的返回值。
\- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。
\- fd：是需要监听的fd（文件描述符）
\- epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：

```
struct epoll_event {
  __uint32_t events;  /* Epoll events */
  epoll_data_t data;  /* User data variable */
};

//events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
```

**3. int epoll_wait(int epfd, struct epoll_event \* events, int maxevents, int timeout);**
等待epfd上的io事件，最多返回maxevents个事件。
参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。

**epoll没有对应的fd限制，并且线程安全，能够告诉具体哪个socket有数据
**等待epfd上的io事件，最多返回maxevents个事件。
参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。

**epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：
　　LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。
　　ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。
**
epoll总结
在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。)

#### 零拷贝机制：

[这是三歪看过最好的零拷贝Zero-Copy文章了！](https://mp.weixin.qq.com/s/LAWUHrRSnxKKicHz1FiGVw)[这个零拷贝的原理讲解的确实很好]

Linux系统中一切皆文件，仔细想一下Linux系统的很多活动无外乎**读操作**和**写操作**，零拷贝就是为了提高读写性能而出现的

mmap：内存映射文件的机制，它实现了将内存中读缓存区地址和用户空间缓存区地址进行映射，从而实现内核缓冲区与用户缓冲区的共享。

​		mmap确实很是容易让人晕：[Linux 中 mmap() 函数的内存映射问题理解？(知乎高分解答)](https://www.zhihu.com/question/48161206)

sendfile：sendfile系统调用是在 Linux 内核2.1版本中被引入，它建立了两个文件之间的传输通道。

sendfile方式只使用一个函数就可以完成之前的read+write 和 mmap+write的功能，这样就少了2次状态切换，由于数据不经过用户缓冲区，因此该数据无法被修改。



##### Kafka的零拷贝机制实现：

​		[Kafka的零拷贝机制的详细介绍](https://zhuanlan.zhihu.com/p/78335525)

​		Producer生产的数据持久化到broker，才用mmap文件映射，实现顺序写入

​		Customer从broker读取数据，采用sendfile，将磁盘文件读到OS内核缓冲区后，直接转到socket buffer进行网络发送。

##### Netty的零拷贝机制实现：

[Netty对零拷贝(Zero Copy)三个层次的实现](https://zhuanlan.zhihu.com/p/88599349)

1，避免数据流经过用户空间
即netty通过使用FileRegion类的transferTo（）方法，可以直接将对应的内核读缓存直接到内核socket写缓存（）
2，避免数据从JVM 堆内存到C head的拷贝
netty使用了**堆外内存**的策略来避免其JVM堆内存到Chead 的拷贝
3，减少数据在用户空间的多次拷贝
Netty使用是提供了CompositeByteBuf类，它提供了对多个ByteBuffer的一个"视图"，可以将它们逻辑上当成一个完整的ByteBuffer来操作，这样就免去了重新分配空间再复制数据的开销。

### 场景分析

#### 如何通过并发调用多个RPC接口

这个问题有点模糊，

对于多个rpc接口（这里我们就把对应的rpc接口当作dubbo接口）

**如何通过并发调用多个RPC接口想成对应的如何通过并发调用多个Dubbo接口，并且对接口进行聚合汇总的操作**

1，首先对于调用多个接口并且对数据进行汇总操作的时候，如果我们一个一个的进行调用，必然是会影响到对应的性能，（性能资源串行化，一个一个的调用肯定是不合适的）

2，使用线程池的策略，并通过CountDownLock的策略，保持所有的线程任务都能一起并发执行完毕，（这个问题也仅仅适用于对应的多个RPC接口的互不影响性，换句话说，接口之前没有对应的依赖关系）

3，对于多个rpc接口存在对应的依赖关系（面试的时候你如果说把rpc接口的依赖关系弄成一个rpc接口肯定是不合适的）,需要结合线程池和**CompletableFuture** 进行实现

**CompleteFuture的具体场景：异步任务完成时回调 线程串行化方案 ，两任务组合-都要完成 两任务组合-任意一个完成**

4，考虑RPC的容错机制了，因为你要具体的场景分析，然后分析对应的具体的RPC的容错策略：

#### 如何排查线上OM的问题